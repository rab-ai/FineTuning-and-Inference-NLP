{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import transformers\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LENGTH = 128\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.1\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath, drop_na=True):\n",
    "    \"\"\"Loads data, checks for missing values, and optionally drops missing rows.\"\"\"\n",
    "    data = pd.read_csv(filepath, sep='\\t')\n",
    "    print(f\"Dataset Statistics for {filepath}:\")\n",
    "    print(data['label'].value_counts(normalize=True))\n",
    "    print(f\"Missing Value Check for {filepath}:\")\n",
    "    print(data.isnull().sum())\n",
    "    if drop_na:\n",
    "        data = data.dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(data, test_size=TRAIN_TEST_SPLIT_RATIO, random_state=RANDOM_STATE):\n",
    "    \"\"\"Splits the dataset into training and test sets with stratification.\"\"\"\n",
    "    train, test = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        stratify=data['label'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    print(\"Class Distribution in Training Set:\")\n",
    "    print(train['label'].value_counts(normalize=True))\n",
    "    print(\"Class Distribution in Test Set:\")\n",
    "    print(test['label'].value_counts(normalize=True))\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(train, test, train_filepath, test_filepath):\n",
    "    \"\"\"Saves the training and test sets to CSV files.\"\"\"\n",
    "    train.to_csv(train_filepath, index=False)\n",
    "    test.to_csv(test_filepath, index=False)\n",
    "    print(f\"Saved training set to {train_filepath}\")\n",
    "    print(f\"Saved test set to {test_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets_for_training(train_data, test_data, tokenizer):\n",
    "    \"\"\"Tokenizes the data and prepares it for training.\"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text_en\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_data).map(tokenize_function, batched=True)\n",
    "    test_dataset = Dataset.from_pandas(test_data).map(tokenize_function, batched=True)\n",
    "\n",
    "    train_dataset = train_dataset.remove_columns([\"id\", \"speaker\", \"sex\", \"text\", \"text_en\"])\n",
    "    test_dataset = test_dataset.remove_columns([\"id\", \"speaker\", \"sex\", \"text\", \"text_en\"])\n",
    "\n",
    "    train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "    test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, test_dataset, tokenizer):\n",
    "    \"\"\"Trains a BERT model and evaluates it.\"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_task2\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=50\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", results)\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainer, test_dataset):\n",
    "    \"\"\"Calculates accuracy and F1 score of the model.\"\"\"\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    preds = predictions.predictions.argmax(axis=-1)\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics for power-tr-train.tsv:\n",
      "label\n",
      "1    0.513806\n",
      "0    0.486194\n",
      "Name: proportion, dtype: float64\n",
      "Missing Value Check for power-tr-train.tsv:\n",
      "id         0\n",
      "speaker    0\n",
      "sex        0\n",
      "text       0\n",
      "text_en    0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "power_data = load_and_preprocess_data(\"power-tr-train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in Training Set:\n",
      "label\n",
      "1    0.513774\n",
      "0    0.486226\n",
      "Name: proportion, dtype: float64\n",
      "Class Distribution in Test Set:\n",
      "label\n",
      "1    0.514089\n",
      "0    0.485911\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "power_train, power_test = stratified_split(power_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training set to power_train_processed.csv\n",
      "Saved test set to power_test_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "save_datasets(power_train, power_test, \"power_train_processed.csv\", \"power_test_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer setup\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fcc6221d224a608f1416eaa1c577d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15645 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fe1a2fac274d2f98ffc3c08b8a8010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare datasets for training\n",
    "power_train_dataset, power_test_dataset = prepare_datasets_for_training(power_train, power_test, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\rabai\\AppData\\Local\\Temp\\ipykernel_23400\\1560980363.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eea6f0e3ef8488fbec3b48ccfb5d81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6978, 'grad_norm': 1.3391704559326172, 'learning_rate': 1.9659168370824816e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6521, 'grad_norm': 4.183454990386963, 'learning_rate': 1.9318336741649627e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6196, 'grad_norm': 3.2177321910858154, 'learning_rate': 1.8977505112474438e-05, 'epoch': 0.15}\n",
      "{'loss': 0.5913, 'grad_norm': 5.108887195587158, 'learning_rate': 1.8636673483299253e-05, 'epoch': 0.2}\n",
      "{'loss': 0.5671, 'grad_norm': 10.113988876342773, 'learning_rate': 1.8295841854124064e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5312, 'grad_norm': 7.7452616691589355, 'learning_rate': 1.795501022494888e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5617, 'grad_norm': 6.042710304260254, 'learning_rate': 1.761417859577369e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4899, 'grad_norm': 5.283132076263428, 'learning_rate': 1.72733469665985e-05, 'epoch': 0.41}\n",
      "{'loss': 0.5376, 'grad_norm': 7.643433094024658, 'learning_rate': 1.6932515337423315e-05, 'epoch': 0.46}\n",
      "{'loss': 0.4834, 'grad_norm': 8.389946937561035, 'learning_rate': 1.6591683708248126e-05, 'epoch': 0.51}\n",
      "{'loss': 0.514, 'grad_norm': 5.100094318389893, 'learning_rate': 1.6250852079072937e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4824, 'grad_norm': 4.297148704528809, 'learning_rate': 1.591002044989775e-05, 'epoch': 0.61}\n",
      "{'loss': 0.4833, 'grad_norm': 6.9862260818481445, 'learning_rate': 1.5569188820722566e-05, 'epoch': 0.66}\n",
      "{'loss': 0.493, 'grad_norm': 4.439032554626465, 'learning_rate': 1.5228357191547376e-05, 'epoch': 0.72}\n",
      "{'loss': 0.4786, 'grad_norm': 5.448586463928223, 'learning_rate': 1.488752556237219e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4903, 'grad_norm': 9.655645370483398, 'learning_rate': 1.4546693933197003e-05, 'epoch': 0.82}\n",
      "{'loss': 0.4608, 'grad_norm': 4.78670072555542, 'learning_rate': 1.4205862304021814e-05, 'epoch': 0.87}\n",
      "{'loss': 0.4443, 'grad_norm': 6.186481952667236, 'learning_rate': 1.3865030674846627e-05, 'epoch': 0.92}\n",
      "{'loss': 0.4289, 'grad_norm': 4.686895847320557, 'learning_rate': 1.352419904567144e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e66ed0c45a4588bf772681c6ed9a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4340490400791168, 'eval_runtime': 14.8085, 'eval_samples_per_second': 117.433, 'eval_steps_per_second': 7.361, 'epoch': 1.0}\n",
      "{'loss': 0.4283, 'grad_norm': 6.718875408172607, 'learning_rate': 1.318336741649625e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3821, 'grad_norm': 15.446608543395996, 'learning_rate': 1.2842535787321065e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3745, 'grad_norm': 4.825876235961914, 'learning_rate': 1.2501704158145878e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3721, 'grad_norm': 11.287981986999512, 'learning_rate': 1.2160872528970689e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3524, 'grad_norm': 10.542778015136719, 'learning_rate': 1.1820040899795502e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3744, 'grad_norm': 10.010162353515625, 'learning_rate': 1.1479209270620315e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3746, 'grad_norm': 6.4129228591918945, 'learning_rate': 1.1138377641445126e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3448, 'grad_norm': 7.605463981628418, 'learning_rate': 1.079754601226994e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3424, 'grad_norm': 9.722461700439453, 'learning_rate': 1.0456714383094753e-05, 'epoch': 1.43}\n",
      "{'loss': 0.3616, 'grad_norm': 11.658703804016113, 'learning_rate': 1.0115882753919564e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3241, 'grad_norm': 19.699098587036133, 'learning_rate': 9.775051124744377e-06, 'epoch': 1.53}\n",
      "{'loss': 0.3902, 'grad_norm': 19.397966384887695, 'learning_rate': 9.43421949556919e-06, 'epoch': 1.58}\n",
      "{'loss': 0.3843, 'grad_norm': 6.770761489868164, 'learning_rate': 9.093387866394002e-06, 'epoch': 1.64}\n",
      "{'loss': 0.3204, 'grad_norm': 12.107854843139648, 'learning_rate': 8.752556237218815e-06, 'epoch': 1.69}\n",
      "{'loss': 0.322, 'grad_norm': 8.169734954833984, 'learning_rate': 8.411724608043628e-06, 'epoch': 1.74}\n",
      "{'loss': 0.3616, 'grad_norm': 6.367907524108887, 'learning_rate': 8.070892978868439e-06, 'epoch': 1.79}\n",
      "{'loss': 0.3498, 'grad_norm': 9.40942668914795, 'learning_rate': 7.730061349693252e-06, 'epoch': 1.84}\n",
      "{'loss': 0.3841, 'grad_norm': 7.38999080657959, 'learning_rate': 7.389229720518065e-06, 'epoch': 1.89}\n",
      "{'loss': 0.3392, 'grad_norm': 4.832126617431641, 'learning_rate': 7.048398091342877e-06, 'epoch': 1.94}\n",
      "{'loss': 0.3196, 'grad_norm': 13.264775276184082, 'learning_rate': 6.707566462167689e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57fbb9a60cf4d3dab3d52eb9cfe67ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4357866942882538, 'eval_runtime': 14.7162, 'eval_samples_per_second': 118.169, 'eval_steps_per_second': 7.407, 'epoch': 2.0}\n",
      "{'loss': 0.2945, 'grad_norm': 6.184322834014893, 'learning_rate': 6.366734832992503e-06, 'epoch': 2.04}\n",
      "{'loss': 0.2377, 'grad_norm': 6.779289722442627, 'learning_rate': 6.025903203817315e-06, 'epoch': 2.1}\n",
      "{'loss': 0.2449, 'grad_norm': 8.739617347717285, 'learning_rate': 5.685071574642127e-06, 'epoch': 2.15}\n",
      "{'loss': 0.2649, 'grad_norm': 14.639701843261719, 'learning_rate': 5.3442399454669404e-06, 'epoch': 2.2}\n",
      "{'loss': 0.2726, 'grad_norm': 7.447010040283203, 'learning_rate': 5.003408316291752e-06, 'epoch': 2.25}\n",
      "{'loss': 0.2405, 'grad_norm': 10.573034286499023, 'learning_rate': 4.662576687116564e-06, 'epoch': 2.3}\n",
      "{'loss': 0.2668, 'grad_norm': 9.638249397277832, 'learning_rate': 4.321745057941377e-06, 'epoch': 2.35}\n",
      "{'loss': 0.239, 'grad_norm': 8.02473258972168, 'learning_rate': 3.98091342876619e-06, 'epoch': 2.4}\n",
      "{'loss': 0.2478, 'grad_norm': 5.467612266540527, 'learning_rate': 3.6400817995910027e-06, 'epoch': 2.45}\n",
      "{'loss': 0.2066, 'grad_norm': 10.068446159362793, 'learning_rate': 3.2992501704158146e-06, 'epoch': 2.51}\n",
      "{'loss': 0.2764, 'grad_norm': 10.72965145111084, 'learning_rate': 2.9584185412406274e-06, 'epoch': 2.56}\n",
      "{'loss': 0.2204, 'grad_norm': 9.397408485412598, 'learning_rate': 2.61758691206544e-06, 'epoch': 2.61}\n",
      "{'loss': 0.243, 'grad_norm': 7.648974418640137, 'learning_rate': 2.276755282890252e-06, 'epoch': 2.66}\n",
      "{'loss': 0.2545, 'grad_norm': 7.730958938598633, 'learning_rate': 1.935923653715065e-06, 'epoch': 2.71}\n",
      "{'loss': 0.2246, 'grad_norm': 17.306442260742188, 'learning_rate': 1.5950920245398775e-06, 'epoch': 2.76}\n",
      "{'loss': 0.2062, 'grad_norm': 6.329063892364502, 'learning_rate': 1.25426039536469e-06, 'epoch': 2.81}\n",
      "{'loss': 0.2377, 'grad_norm': 18.959396362304688, 'learning_rate': 9.134287661895025e-07, 'epoch': 2.86}\n",
      "{'loss': 0.2213, 'grad_norm': 7.433547019958496, 'learning_rate': 5.72597137014315e-07, 'epoch': 2.91}\n",
      "{'loss': 0.235, 'grad_norm': 11.995397567749023, 'learning_rate': 2.317655078391275e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e9dfd599a44e59861e6816b3f7922d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5161563754081726, 'eval_runtime': 14.8866, 'eval_samples_per_second': 116.816, 'eval_steps_per_second': 7.322, 'epoch': 3.0}\n",
      "{'train_runtime': 8151.3009, 'train_samples_per_second': 5.758, 'train_steps_per_second': 0.36, 'train_loss': 0.3748231405454504, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2f51d1d4f0429eab1e3aa3d1394495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.4340490400791168, 'eval_runtime': 15.1618, 'eval_samples_per_second': 114.696, 'eval_steps_per_second': 7.189, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94303b79e9c411f8fa3ea4a513925b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7918\n",
      "F1 Score: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7918343875790684, 0.7912730486144101)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "model, trainer = train_model(power_train_dataset, power_test_dataset, tokenizer)\n",
    "evaluate_model(trainer, power_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./task2_multilingual_bert\")\n",
    "tokenizer.save_pretrained(\"./task2_multilingual_bert\")\n",
    "print(\"Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the test dataset for Task 2\n",
    "power_test = pd.read_csv(\"power_test_processed.csv\")\n",
    "\n",
    "# Initialize the causal inference pipeline\n",
    "causal_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"bigscience/bloomz-1b1\",\n",
    "    device=0  # Use GPU for inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit input text to the first 500 characters to reduce memory usage\n",
    "def limit_text_length(dataset, column_name=\"text_en\"):\n",
    "    return dataset.map(lambda x: {column_name: x[column_name][:500]})\n",
    "\n",
    "# Generate prompts and perform inference in batches\n",
    "def generate_texts(batch, column_name=\"text_en\", prompt_template=\"Classify the text\"):\n",
    "    prompts = [prompt_template.format(text=text) for text in batch[column_name]]\n",
    "    outputs = causal_pipeline(prompts, max_new_tokens=10)\n",
    "    return {\"predictions\": [output[0][\"generated_text\"].strip() for output in outputs]}\n",
    "\n",
    "# Process predictions into binary labels\n",
    "def process_predictions(predictions):\n",
    "    binary_results = []\n",
    "    for pred in predictions:\n",
    "        if \"0\" in pred:\n",
    "            binary_results.append(0)\n",
    "        elif \"1\" in pred:\n",
    "            binary_results.append(1)\n",
    "        else:\n",
    "            binary_results.append(None)  # Model uncertain\n",
    "    return binary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d728dfd21e4f0090444a55e7e47d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9531dd4a17e49aaa73238005c489d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m turkish_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAşağıdaki meclis konuşmasına dayanarak, konuşmacının partisinin hükümette (0) ya da muhalefette (1) olduğunu belirtiniz:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCevap:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m results_turkish \u001b[38;5;241m=\u001b[39m \u001b[43mturkish_test_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturkish_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Process binary predictions for Turkish texts\u001b[39;00m\n\u001b[0;32m     18\u001b[0m binary_predictions_turkish \u001b[38;5;241m=\u001b[39m process_predictions(results_turkish[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3073\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:3476\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3472\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3473\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3474\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3476\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:3338\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3337\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3338\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3340\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3341\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3342\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      8\u001b[0m turkish_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAşağıdaki meclis konuşmasına dayanarak, konuşmacının partisinin hükümette (0) ya da muhalefette (1) olduğunu belirtiniz:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCevap:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[0;32m     11\u001b[0m results_turkish \u001b[38;5;241m=\u001b[39m turkish_test_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m batch: \u001b[43mgenerate_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturkish_prompt\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     13\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Process binary predictions for Turkish texts\u001b[39;00m\n\u001b[0;32m     18\u001b[0m binary_predictions_turkish \u001b[38;5;241m=\u001b[39m process_predictions(results_turkish[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mgenerate_texts\u001b[1;34m(batch, column_name, prompt_template)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_texts\u001b[39m(batch, column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassify the text\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m [prompt_template\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m batch[column_name]]\n\u001b[1;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: [output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py:1282\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1279\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1280\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1281\u001b[0m     )\n\u001b[1;32m-> 1282\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\base.py:1208\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1207\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1208\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\pipelines\\text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2249\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2272\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:3240\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3237\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[0;32m   3239\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3240\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[0;32m   3241\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[0;32m   3242\u001b[0m ):\n\u001b[0;32m   3243\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   3244\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert the dataset to HuggingFace Dataset format\n",
    "turkish_test_dataset = Dataset.from_pandas(power_test)\n",
    "\n",
    "# Limit text length for Turkish column\n",
    "turkish_test_dataset = limit_text_length(turkish_test_dataset, column_name=\"text\")\n",
    "\n",
    "# Define the Turkish prompt\n",
    "turkish_prompt = \"Aşağıdaki meclis konuşmasına dayanarak, konuşmacının partisinin hükümette (0) ya da muhalefette (1) olduğunu belirtiniz:\\n\\n{text}\\n\\nCevap:\"\n",
    "\n",
    "# Perform inference\n",
    "results_turkish = turkish_test_dataset.map(\n",
    "    lambda batch: generate_texts(batch, column_name=\"text\", prompt_template=turkish_prompt),\n",
    "    batched=True,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Process binary predictions for Turkish texts\n",
    "binary_predictions_turkish = process_predictions(results_turkish[\"predictions\"])\n",
    "results_turkish = results_turkish.add_column(\"binary_predictions\", binary_predictions_turkish)\n",
    "\n",
    "# Calculate accuracy for Turkish predictions\n",
    "correct_predictions_turkish = [\n",
    "    binary == label for binary, label in zip(results_turkish[\"binary_predictions\"], results_turkish[\"label\"])\n",
    "]\n",
    "accuracy_turkish = sum(correct_predictions_turkish) / len(correct_predictions_turkish)\n",
    "\n",
    "# Save results to CSV\n",
    "results_turkish_df = results_turkish.to_pandas()\n",
    "results_turkish_df.to_csv(\"task2_bloomz_turkish_results_binary.csv\", index=False)\n",
    "\n",
    "print(f\"Zero-Shot Inference Accuracy (Turkish - Task 2): {accuracy_turkish:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343953d51d9b42b1bb9012048753a62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x000001CBFF6A8540> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777bc2130c524ec8a3bb6e6bca70f94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664c3e917b394451b27e3034c3b9f360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Inference Accuracy (English): 41.82%\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset to HuggingFace Dataset format\n",
    "english_test_dataset = Dataset.from_pandas(power_test)\n",
    "\n",
    "# Limit text length for English column\n",
    "english_test_dataset = limit_text_length(english_test_dataset, column_name=\"text_en\")\n",
    "\n",
    "# Define the English prompt\n",
    "english_prompt = \"Based on the following parliamentary speech, classify whether the speaker's party is governing (0) or opposition (1):\\n\\n{text}\\n\\nAnswer:\"\n",
    "\n",
    "# Perform inference\n",
    "results_english = english_test_dataset.map(\n",
    "    lambda batch: generate_texts(batch, column_name=\"text_en\", prompt_template=english_prompt),\n",
    "    batched=True,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# Process binary predictions for English texts\n",
    "binary_predictions_english = process_predictions(results_english[\"predictions\"])\n",
    "results_english = results_english.add_column(\"binary_predictions\", binary_predictions_english)\n",
    "\n",
    "# Calculate accuracy for English predictions\n",
    "correct_predictions_english = [\n",
    "    binary == label for binary, label in zip(results_english[\"binary_predictions\"], results_english[\"label\"])\n",
    "]\n",
    "accuracy_english = sum(correct_predictions_english) / len(correct_predictions_english)\n",
    "\n",
    "# Save results to CSV\n",
    "results_english_df = results_english.to_pandas()\n",
    "results_english_df.to_csv(\"task2_bloomz_english_results_binary.csv\", index=False)\n",
    "\n",
    "print(f\"Zero-Shot Inference Accuracy (English - Task 2): {accuracy_english:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
